#!/usr/bin/env python3
import os
import json
import requests
import pdfplumber
from datetime import datetime
import re
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

TOHAN_PDF_URL = "https://www.tohan.jp/wp/wp-content/uploads/2026/02/202601.pdf"

GENRES = [
    "æ–‡èŠ¸æ›¸",
    "ãƒãƒ³ãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³ãƒ»ãƒ©ã‚¤ãƒˆã‚¨ãƒƒã‚»ã‚¤",
    "ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ",
    "ãƒ“ã‚¸ãƒã‚¹æ›¸",
    "è¶£å‘³å®Ÿç”¨æ›¸",
    "ç”Ÿæ´»å®Ÿç”¨æ›¸",
    "å…ç«¥æ›¸",
    "ãƒãƒ™ãƒ«ã‚¹",
    "æ–°æ›¸",
    "æ–‡åº«",
    "ã‚³ãƒŸãƒƒã‚¯ã‚¹"
]

def download_tohan_pdf(url):
    """Download Tohan PDF"""
    print("ğŸ“¥ Downloading Tohan PDF...")
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        pdf_path = '/tmp/tohan.pdf'
        with open(pdf_path, 'wb') as f:
            f.write(response.content)
        
        print(f"âœ… PDF downloaded ({len(response.content)} bytes)\n")
        return pdf_path
    
    except Exception as e:
        logger.error(f"Error downloading PDF: {e}")
        return None

def parse_tohan_pdf(pdf_path):
    """Parse Tohan PDF and extract rankings from text"""
    print("ğŸ“– Parsing Tohan PDF...\n")
    
    data = {
        "updated": datetime.now().isoformat() + "Z",
        "source": "tohan.jp",
        "genres": {}
    }
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"Total pages: {len(pdf.pages)}\n")
            
            # Extract all text
            full_text = ""
            for page in pdf.pages:
                full_text += page.extract_text() + "\n"
            
            # Parse each genre
            for genre in GENRES:
                print(f"ğŸ” Extracting ã€{genre}ã€‘...")
                
                genre_pattern = f"ã€{genre}ã€‘"
                
                if genre_pattern not in full_text:
                    print(f"   âš ï¸  Not found\n")
                    continue
                
                # Find start and end of genre section
                start_idx = full_text.find(genre_pattern)
                
                end_idx = len(full_text)
                for next_genre in GENRES:
                    if next_genre != genre:
                        next_idx = full_text.find(f"ã€{next_genre}ã€‘", start_idx + 1)
                        if next_idx != -1 and next_idx < end_idx:
                            end_idx = next_idx
                
                genre_section = full_text[start_idx:end_idx]
                
                # Parse books
                books = parse_genre_section(genre_section)
                
                if books:
                    data["genres"][genre] = books
                    print(f"   âœ… {len(books)} books extracted\n")
                else:
                    print(f"   âš ï¸  No books found\n")
        
        return data
    
    except Exception as e:
        logger.error(f"Error parsing PDF: {e}")
        import traceback
        traceback.print_exc()
        return None

def parse_genre_section(section_text):
    """Parse a genre section and extract books"""
    books = []
    lines = [line.rstrip() for line in section_text.split('\n')]
    
    # Find header line with column names (if it exists)
    header_idx = -1
    for i, line in enumerate(lines):
        if 'æ›¸' in line and 'å' in line:
            header_idx = i
            break
    
    # If no header found, start from first line after genre marker
    if header_idx == -1:
        # Find the genre marker line
        for i, line in enumerate(lines):
            if 'ã€' in line and 'ã€‘' in line:
                header_idx = i
                break
    
    if header_idx == -1:
        return []
    
    # Parse data lines starting after header/genre marker
    i = header_idx + 1
    while i < len(lines) and len(books) < 10:
        line = lines[i]
        
        # Look for rank (1-10) at start
        match = re.match(r'^(\d+)\s+', line)
        
        if not match:
            i += 1
            continue
        
        rank = int(match.group(1))
        # Only process ranks 1-10
        if rank < 1 or rank > 10:
            i += 1
            continue
        
        # Collect all lines for this book entry
        book_data = [line]
        i += 1
        
        # Continue until next rank or end
        while i < len(lines):
            next_line = lines[i]
            
            # Stop at next rank or genre
            if re.match(r'^(\d+)\s+', next_line) or 'ã€' in next_line:
                break
            
            # Stop at page marker or section separator
            if 'ãƒˆãƒ¼ãƒãƒ³' in next_line:
                i += 1
                break
            
            # Skip empty lines but continue
            if next_line.strip():
                book_data.append(next_line)
            
            i += 1
        
        # Parse the book data
        book = parse_book_entry(book_data, rank)
        if book:
            books.append(book)
            print(f"      âœ“ Rank {rank}: {book['title']}")
    
    return books

# List of known publishers
# List of known publishers
PUBLISHERS = [
    "SBã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–", "KADOKAWA", "å¹¸ç¦ã®ç§‘å­¦å‡ºç‰ˆ", "ã‚ªãƒ¬ãƒ³ã‚¸ãƒšãƒ¼ã‚¸", "å°å­¦é¤¨",
    "ç¥å®®é¤¨", "Gakken", "æœæ—¥æ–°èå‡ºç‰ˆ", "ãƒ¯ãƒ³ï½¥ãƒ‘ãƒ–ãƒªãƒƒã‚·ãƒ³ã‚°", "å¹»å†¬èˆ",
    "æ—¥æœ¬çµŒæ¸ˆæ–°èå‡ºç‰ˆ", "é«˜æ©‹æ›¸åº—", "ã¨ãã‚ç·åˆã‚µãƒ¼ãƒ“ã‚¹", "ã‚µãƒ³ã‚¯ãƒãƒ¥ã‚¢ãƒªå‡ºç‰ˆ",
    "1ä¸‡å¹´å ‚å‡ºç‰ˆ", "PHPç ”ç©¶æ‰€", "æ¯æ—¥æ–°èå‡ºç‰ˆ", "æ—¥çµŒBP", "ãƒ–ãƒ©ã‚¦ãƒ³ã‚ºãƒ–ãƒƒã‚¯ã‚¹",
    "ã‚¹ã‚¤ãƒƒãƒï½¥ãƒ‘ãƒ–ãƒªãƒƒã‚·ãƒ³ã‚°", "ã™ã°ã‚‹èˆ", "ã‚µãƒ³ãƒãƒ¼ã‚¯å‡ºç‰ˆ", "ãƒ¯ãƒ‹ãƒ–ãƒƒã‚¯ã‚¹",
    "ãƒã‚¬ã‚¸ãƒ³ãƒã‚¦ã‚¹", "ç¦éŸ³é¤¨æ›¸åº—", "å²©å´æ›¸åº—", "ãƒãƒ¼ãƒ‘ãƒ¼ã‚³ãƒªãƒ³ã‚ºï½¥ã‚¸ãƒ£ãƒ‘ãƒ³",
    "æ–‡è—æ˜¥ç§‹", "æ–°æ½®ç¤¾", "åŒè‘‰ç¤¾", "é£›é³¥æ–°ç¤¾", "è¬›è«‡ç¤¾", "æ±äº¬å‰µå…ƒç¤¾",
    "å®å³¶ç¤¾", "ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ç¤¾", "æ±æ´‹çµŒæ¸ˆæ–°å ±ç¤¾", "æœæ—¥æ–°èå‡ºç‰ˆ", "æ–°æ˜Ÿå‡ºç‰ˆç¤¾",
    "ä¸­å¤®å…¬è«–æ–°ç¤¾", "é›†è‹±ç¤¾", "å…‰æ–‡ç¤¾", "ã‚¯ãƒ©ãƒ¼ã‚±ãƒ³ã‚³ãƒŸãƒƒã‚¯ã‚¹", "NHKå‡ºç‰ˆ"
]

def parse_book_entry(lines, rank):
    """Parse a single book entry from multiple lines"""
    
    # Join all lines
    full_text = ' '.join(line.strip() for line in lines if line.strip())
    
    # Remove rank number from start
    full_text = re.sub(r'^(\d+)\s+', '', full_text).strip()
    
    # Remove dash characters (â€•)
    full_text = full_text.replace('â€•', '').replace('â€•â€•â€•â€•â€•â€•â€•â€•', '').strip()
    full_text = re.sub(r'\s+', ' ', full_text).strip()
    
    # Extract ISBN (starts with 978 and has digits/dashes)
    isbn = ""
    isbn_match = re.search(r'(978[\d\-]{10,})', full_text)
    if isbn_match:
        isbn = isbn_match.group(1)
        full_text = full_text[:isbn_match.start()].strip() + ' ' + full_text[isbn_match.end():].strip()
        full_text = full_text.strip()
    
    # Extract PRICE (3+ digits or digits with commas)
    price = ""
    price_match = re.search(r'\b([\d,]{3,}|\d{3,})\b(?![\d\-])', full_text)
    if price_match:
        price = price_match.group(1)
        full_text = full_text[:price_match.start()].strip() + ' ' + full_text[price_match.end():].strip()
        full_text = full_text.strip()
    
    # Extract AUTHOR - always contains ï¼
    # Can be: Nameï¼è‘—, Nameï¼ç·¨è‘—, Nameï¼ä½œ, Nameï¼åŸä½œ, Nameï¼æ¼«ç”», etc.
    # Multiple authors: Name1ï¼è‘—ã€€Name2ï¼è‘— or Name1ï¼è‘—ã€€Name2ï¼æ¼«ç”»
    author = ""
    # Match: text with ï¼ followed by è‘—/ç·¨è‘—/ä½œ/åŸä½œ/æ¼«ç”»/ç·¨/è¨³/ç›£ä¿®/ã‚¤ãƒ©ã‚¹ãƒˆ/ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å”åŠ›
    author_pattern = r'([^\sï¼]+(?:ï¼(?:è‘—|ç·¨è‘—|ä½œ|åŸä½œ|æ¼«ç”»|ç·¨|è¨³|ç›£ä¿®|ã‚¤ãƒ©ã‚¹ãƒˆ|ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å”åŠ›))+(?:\s+[^\sï¼]+(?:ï¼(?:è‘—|ç·¨è‘—|ä½œ|åŸä½œ|æ¼«ç”»|ç·¨|è¨³|ç›£ä¿®|ã‚¤ãƒ©ã‚¹ãƒˆ|ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å”åŠ›))+)*)'
    author_match = re.search(author_pattern, full_text)
    
    if author_match:
        author = author_match.group(1).strip()
        # Remove author from full_text
        full_text = full_text[:author_match.start()].strip() + ' ' + full_text[author_match.end():].strip()
        full_text = full_text.strip()
    
    # Extract PUBLISHER (match known publishers or words ending with ç¤¾)
    publisher = ""
    
    # First, try to match known publishers
    for pub in sorted(PUBLISHERS, key=len, reverse=True):  # Longest first
        if pub in full_text:
            publisher = pub
            # Remove publisher from full_text
            idx = full_text.find(pub)
            full_text = full_text[:idx].strip() + ' ' + full_text[idx + len(pub):].strip()
            full_text = full_text.strip()
            break
    
    # If no known publisher found, look for word ending with ç¤¾
    if not publisher:
        society_match = re.search(r'(\S+ç¤¾)', full_text)
        if society_match:
            publisher = society_match.group(1)
            # Remove publisher from full_text
            full_text = full_text[:society_match.start()].strip() + ' ' + full_text[society_match.end():].strip()
            full_text = full_text.strip()
    
    # What remains is the TITLE
    title = full_text.strip()
    
    # Clean up
    title = re.sub(r'\s+', ' ', title).strip()
    author = re.sub(r'\s+', ' ', author).strip()
    publisher = re.sub(r'\s+', ' ', publisher).strip()
    
    if not title:
        return None
    
    return {
        "rank": rank,
        "title": title,
        "author": author if author else "-",
        "publisher": publisher if publisher else "-",
        "price": price if price else "-",
        "isbn": isbn if isbn else "-"
    }

def correct_overall_from_other_genres(data):
    """Not needed - OVERALL tab removed"""
    return data
    
    overall_books = data["genres"]["ç·åˆ"]
    
    # Build a reference map from all other genres
    reference_map = {}  # title -> book data
    
    for genre, books in data["genres"].items():
        if genre == "ç·åˆ":
            continue
        
        for book in books:
            title = book["title"].lower().strip()
            # Use this genre's data as reference if not already set
            if title not in reference_map:
                reference_map[title] = book.copy()
    
    # Correct overall books using reference map
    corrected_books = []
    for book in overall_books:
        title = book["title"].lower().strip()
        
        # Check if this title exists in other genres
        if title in reference_map:
            ref_book = reference_map[title]
            # Use reference data to fill in missing fields
            corrected = {
                "rank": book["rank"],
                "title": ref_book["title"],  # Use correct title from other genre
                "author": ref_book["author"] if ref_book["author"] != "-" else book["author"],
                "publisher": ref_book["publisher"] if ref_book["publisher"] != "-" else book["publisher"],
                "price": ref_book["price"] if ref_book["price"] != "-" else book["price"],
                "isbn": ref_book["isbn"] if ref_book["isbn"] != "-" else book["isbn"]
            }
            corrected_books.append(corrected)
        else:
            # Keep original if no reference found
            corrected_books.append(book)
    
    data["genres"]["ç·åˆ"] = corrected_books
    return data

def main():
    print("ğŸ“š Starting Tohan PDF Scraper...\n")
    
    pdf_path = download_tohan_pdf(TOHAN_PDF_URL)
    if not pdf_path:
        return
    
    data = parse_tohan_pdf(pdf_path)
    if not data:
        return
    
    # Correct OVERALL genre using other genres
    data = correct_overall_from_other_genres(data)
    
    # Save data.js
    try:
        js_content = f"const oricon_data = {json.dumps(data, ensure_ascii=False, indent=2)};\n"
        
        with open('data.js', 'w', encoding='utf-8') as f:
            f.write(js_content)
        
        print(f"âœ… Successfully saved data.js")
        print(f"ğŸ“Š Total genres: {len(data['genres'])}")
        
        total_books = 0
        for genre, books in data['genres'].items():
            print(f"   - {genre}: {len(books)} books")
            total_books += len(books)
        
        print(f"\nğŸ“ˆ Total books scraped: {total_books}")
    
    except Exception as e:
        logger.error(f"Error saving data.js: {e}")
    
    # Cleanup
    if os.path.exists(pdf_path):
        os.remove(pdf_path)

if __name__ == "__main__":
    main()
