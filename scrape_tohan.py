#!/usr/bin/env python3
import os
import json
import requests
import pdfplumber
from datetime import datetime
import re
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

TOHAN_PDF_URL = "https://www.tohan.jp/wp/wp-content/uploads/2026/02/202601.pdf"

GENRES = [
    "ÊñáËä∏Êõ∏",
    "„Éé„É≥„Éï„Ç£„ÇØ„Ç∑„Éß„É≥„Éª„É©„Ç§„Éà„Ç®„ÉÉ„Çª„Ç§",
    "„Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É°„É≥„Éà",
    "ÂÖêÁ´•Êõ∏",
    "„Éé„Éô„É´„Çπ",
    "Êñ∞Êõ∏",
    "ÊñáÂ∫´",
    "„Ç≥„Éü„ÉÉ„ÇØ„Çπ"
]

def download_tohan_pdf(url):
    """Download Tohan PDF"""
    print("üì• Downloading Tohan PDF...")
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        pdf_path = '/tmp/tohan.pdf'
        with open(pdf_path, 'wb') as f:
            f.write(response.content)
        
        print(f"‚úÖ PDF downloaded ({len(response.content)} bytes)\n")
        return pdf_path
    
    except Exception as e:
        logger.error(f"Error downloading PDF: {e}")
        return None

def parse_tohan_pdf(pdf_path):
    """Parse Tohan PDF and extract rankings from text"""
    print("üìñ Parsing Tohan PDF...\n")
    
    data = {
        "updated": datetime.now().isoformat() + "Z",
        "source": "tohan.jp",
        "genres": {}
    }
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"Total pages: {len(pdf.pages)}\n")
            
            # Extract all text
            full_text = ""
            for page in pdf.pages:
                full_text += page.extract_text() + "\n"
            
            # Parse each genre
            for genre in GENRES:
                print(f"üîç Extracting „Äê{genre}„Äë...")
                
                genre_pattern = f"„Äê{genre}„Äë"
                
                if genre_pattern not in full_text:
                    print(f"   ‚ö†Ô∏è  Not found\n")
                    continue
                
                # Find start and end of genre section
                start_idx = full_text.find(genre_pattern)
                
                end_idx = len(full_text)
                for next_genre in GENRES:
                    if next_genre != genre:
                        next_idx = full_text.find(f"„Äê{next_genre}„Äë", start_idx + 1)
                        if next_idx != -1 and next_idx < end_idx:
                            end_idx = next_idx
                
                genre_section = full_text[start_idx:end_idx]
                
                # Parse books
                books = parse_genre_section(genre_section)
                
                if books:
                    data["genres"][genre] = books
                    print(f"   ‚úÖ {len(books)} books extracted\n")
                else:
                    print(f"   ‚ö†Ô∏è  No books found\n")
        
        return data
    
    except Exception as e:
        logger.error(f"Error parsing PDF: {e}")
        import traceback
        traceback.print_exc()
        return None

def parse_genre_section(section_text):
    """Parse a genre section and extract books"""
    books = []
    lines = [line.rstrip() for line in section_text.split('\n')]
    
    # Find header line with column names (if it exists)
    header_idx = -1
    for i, line in enumerate(lines):
        if 'Êõ∏' in line and 'Âêç' in line:
            header_idx = i
            break
    
    # If no header found, start from first line after genre marker
    if header_idx == -1:
        # Find the genre marker line
        for i, line in enumerate(lines):
            if '„Äê' in line and '„Äë' in line:
                header_idx = i
                break
    
    if header_idx == -1:
        return []
    
    # Parse data lines starting after header/genre marker
    i = header_idx + 1
    found_books = 0
    
    while i < len(lines) and found_books < 10:
        line = lines[i].strip()
        
        # Skip empty lines at the beginning
        if not line:
            i += 1
            continue
        
        # Look for rank (1-10) at start
        match = re.match(r'^(\d+)\s+', line)
        
        if not match:
            i += 1
            continue
        
        rank = int(match.group(1))
        # Only process ranks 1-10, in order
        if rank != found_books + 1:
            i += 1
            continue
        
        # Collect all lines for this book entry
        book_data = [line]
        i += 1
        
        # Continue until next rank or end
        while i < len(lines):
            next_line = lines[i]
            
            # Stop at next rank
            if re.match(r'^(\d+)\s+', next_line):
                break
            
            # Stop at genre marker
            if '„Äê' in next_line and '„Äë' in next_line:
                break
            
            # Stop at page marker
            if '„Éà„Éº„Éè„É≥' in next_line or 'ÊúàÈñì„Éô„Çπ„Éà„Çª„É©„Éº' in next_line:
                break
            
            # Add non-empty lines
            if next_line.strip():
                book_data.append(next_line)
            
            i += 1
        
        # Parse the book data
        book = parse_book_entry(book_data, rank)
        if book:
            books.append(book)
            found_books += 1
            print(f"      ‚úì Rank {rank}: {book['title'][:50]}")
    
    return books

# List of known publishers
# List of known publishers
PUBLISHERS = [
    "SB„ÇØ„É™„Ç®„Ç§„ÉÜ„Ç£„Éñ", "KADOKAWA", "Âπ∏Á¶è„ÅÆÁßëÂ≠¶Âá∫Áâà", "„Ç™„É¨„É≥„Ç∏„Éö„Éº„Ç∏", "Â∞èÂ≠¶È§®",
    "Á•ûÂÆÆÈ§®", "Gakken", "ÊúùÊó•Êñ∞ËÅûÂá∫Áâà", "„ÉØ„É≥ÔΩ•„Éë„Éñ„É™„ÉÉ„Ç∑„É≥„Ç∞", "ÂπªÂÜ¨Ëàé",
    "Êó•Êú¨ÁµåÊ∏àÊñ∞ËÅûÂá∫Áâà", "È´òÊ©ãÊõ∏Â∫ó", "„Å®„Åç„ÇèÁ∑èÂêà„Çµ„Éº„Éì„Çπ", "„Çµ„É≥„ÇØ„ÉÅ„É•„Ç¢„É™Âá∫Áâà",
    "1‰∏áÂπ¥Â†ÇÂá∫Áâà", "PHPÁ†îÁ©∂ÊâÄ", "ÊØéÊó•Êñ∞ËÅûÂá∫Áâà", "Êó•ÁµåBP", "„Éñ„É©„Ç¶„É≥„Ç∫„Éñ„ÉÉ„ÇØ„Çπ",
    "„Çπ„Ç§„ÉÉ„ÉÅÔΩ•„Éë„Éñ„É™„ÉÉ„Ç∑„É≥„Ç∞", "„Åô„Å∞„ÇãËàé", "„Çµ„É≥„Éû„Éº„ÇØÂá∫Áâà", "„ÉØ„Éã„Éñ„ÉÉ„ÇØ„Çπ",
    "„Éû„Ç¨„Ç∏„É≥„Éè„Ç¶„Çπ", "Á¶èÈü≥È§®Êõ∏Â∫ó", "Â≤©Â¥éÊõ∏Â∫ó", "„Éè„Éº„Éë„Éº„Ç≥„É™„É≥„Ç∫ÔΩ•„Ç∏„É£„Éë„É≥",
    "ÊñáËóùÊò•Áßã", "Êñ∞ÊΩÆÁ§æ", "ÂèåËëâÁ§æ", "È£õÈ≥•Êñ∞Á§æ", "Ë¨õË´áÁ§æ", "Êù±‰∫¨ÂâµÂÖÉÁ§æ",
    "ÂÆùÂ≥∂Á§æ", "„ÉÄ„Ç§„É§„É¢„É≥„ÉâÁ§æ", "Êù±Ê¥ãÁµåÊ∏àÊñ∞Â†±Á§æ", "ÊúùÊó•Êñ∞ËÅûÂá∫Áâà", "Êñ∞ÊòüÂá∫ÁâàÁ§æ",
    "‰∏≠Â§ÆÂÖ¨Ë´ñÊñ∞Á§æ", "ÈõÜËã±Á§æ", "ÂÖâÊñáÁ§æ", "„ÇØ„É©„Éº„Ç±„É≥„Ç≥„Éü„ÉÉ„ÇØ„Çπ", "NHKÂá∫Áâà"
]

def parse_book_entry(lines, rank):
    """Parse a single book entry from multiple lines"""
    
    # Join all lines
    full_text = ' '.join(line.strip() for line in lines if line.strip())
    
    # Remove rank number from start
    full_text = re.sub(r'^(\d+)\s+', '', full_text).strip()
    
    # Remove dash characters (‚Äï)
    full_text = full_text.replace('‚Äï', '').replace('‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï', '').strip()
    full_text = re.sub(r'\s+', ' ', full_text).strip()
    
    # Extract ISBN (starts with 978 and has digits/dashes)
    isbn = ""
    isbn_match = re.search(r'(978[\d\-]{10,})', full_text)
    if isbn_match:
        isbn = isbn_match.group(1)
        full_text = full_text[:isbn_match.start()].strip() + ' ' + full_text[isbn_match.end():].strip()
        full_text = full_text.strip()
    
    # Extract PRICE (3+ digits or digits with commas)
    price = ""
    price_match = re.search(r'\b([\d,]{3,}|\d{3,})\b(?![\d\-])', full_text)
    if price_match:
        price = price_match.group(1)
        full_text = full_text[:price_match.start()].strip() + ' ' + full_text[price_match.end():].strip()
        full_text = full_text.strip()
    
    # Extract AUTHOR - always contains Ôºè
    # Can be: NameÔºèËëó, NameÔºèÁ∑®Ëëó, NameÔºè‰Ωú, NameÔºèÂéü‰Ωú, NameÔºèÊº´Áîª, etc.
    # Multiple authors: Name1ÔºèËëó„ÄÄName2ÔºèËëó or Name1ÔºèËëó„ÄÄName2ÔºèÊº´Áîª
    author = ""
    # Match: text with Ôºè followed by Ëëó/Á∑®Ëëó/‰Ωú/Âéü‰Ωú/Êº´Áîª/Á∑®/Ë®≥/Áõ£‰øÆ/„Ç§„É©„Çπ„Éà/„Çπ„Éà„Éº„É™„ÉºÂçîÂäõ
    author_pattern = r'([^\sÔºè]+(?:Ôºè(?:Ëëó|Á∑®Ëëó|‰Ωú|Âéü‰Ωú|Êº´Áîª|Á∑®|Ë®≥|Áõ£‰øÆ|„Ç§„É©„Çπ„Éà|„Çπ„Éà„Éº„É™„ÉºÂçîÂäõ))+(?:\s+[^\sÔºè]+(?:Ôºè(?:Ëëó|Á∑®Ëëó|‰Ωú|Âéü‰Ωú|Êº´Áîª|Á∑®|Ë®≥|Áõ£‰øÆ|„Ç§„É©„Çπ„Éà|„Çπ„Éà„Éº„É™„ÉºÂçîÂäõ))+)*)'
    author_match = re.search(author_pattern, full_text)
    
    if author_match:
        author = author_match.group(1).strip()
        # Remove author from full_text
        full_text = full_text[:author_match.start()].strip() + ' ' + full_text[author_match.end():].strip()
        full_text = full_text.strip()
    
    # Extract PUBLISHER (match known publishers or words ending with Á§æ)
    publisher = ""
    
    # First, try to match known publishers
    for pub in sorted(PUBLISHERS, key=len, reverse=True):  # Longest first
        if pub in full_text:
            publisher = pub
            # Remove publisher from full_text
            idx = full_text.find(pub)
            full_text = full_text[:idx].strip() + ' ' + full_text[idx + len(pub):].strip()
            full_text = full_text.strip()
            break
    
    # If no known publisher found, look for word ending with Á§æ
    if not publisher:
        society_match = re.search(r'(\S+Á§æ)', full_text)
        if society_match:
            publisher = society_match.group(1)
            # Remove publisher from full_text
            full_text = full_text[:society_match.start()].strip() + ' ' + full_text[society_match.end():].strip()
            full_text = full_text.strip()
    
    # What remains is the TITLE
    title = full_text.strip()
    
    # Clean up
    title = re.sub(r'\s+', ' ', title).strip()
    author = re.sub(r'\s+', ' ', author).strip()
    publisher = re.sub(r'\s+', ' ', publisher).strip()
    
    if not title:
        return None
    
    return {
        "rank": rank,
        "title": title,
        "author": author if author else "-",
        "publisher": publisher if publisher else "-",
        "price": price if price else "-",
        "isbn": isbn if isbn else "-"
    }

def correct_overall_from_other_genres(data):
    """Not needed - OVERALL tab removed"""
    return data
    
    overall_books = data["genres"]["Á∑èÂêà"]
    
    # Build a reference map from all other genres
    reference_map = {}  # title -> book data
    
    for genre, books in data["genres"].items():
        if genre == "Á∑èÂêà":
            continue
        
        for book in books:
            title = book["title"].lower().strip()
            # Use this genre's data as reference if not already set
            if title not in reference_map:
                reference_map[title] = book.copy()
    
    # Correct overall books using reference map
    corrected_books = []
    for book in overall_books:
        title = book["title"].lower().strip()
        
        # Check if this title exists in other genres
        if title in reference_map:
            ref_book = reference_map[title]
            # Use reference data to fill in missing fields
            corrected = {
                "rank": book["rank"],
                "title": ref_book["title"],  # Use correct title from other genre
                "author": ref_book["author"] if ref_book["author"] != "-" else book["author"],
                "publisher": ref_book["publisher"] if ref_book["publisher"] != "-" else book["publisher"],
                "price": ref_book["price"] if ref_book["price"] != "-" else book["price"],
                "isbn": ref_book["isbn"] if ref_book["isbn"] != "-" else book["isbn"]
            }
            corrected_books.append(corrected)
        else:
            # Keep original if no reference found
            corrected_books.append(book)
    
    data["genres"]["Á∑èÂêà"] = corrected_books
    return data

def main():
    print("üìö Starting Tohan PDF Scraper...\n")
    
    pdf_path = download_tohan_pdf(TOHAN_PDF_URL)
    if not pdf_path:
        return
    
    data = parse_tohan_pdf(pdf_path)
    if not data:
        return
    
    # Correct OVERALL genre using other genres
    data = correct_overall_from_other_genres(data)
    
    # Save data.js
    try:
        js_content = f"const oricon_data = {json.dumps(data, ensure_ascii=False, indent=2)};\n"
        
        with open('data.js', 'w', encoding='utf-8') as f:
            f.write(js_content)
        
        print(f"‚úÖ Successfully saved data.js")
        print(f"üìä Total genres: {len(data['genres'])}")
        
        total_books = 0
        for genre, books in data['genres'].items():
            print(f"   - {genre}: {len(books)} books")
            total_books += len(books)
        
        print(f"\nüìà Total books scraped: {total_books}")
    
    except Exception as e:
        logger.error(f"Error saving data.js: {e}")
    
    # Cleanup
    if os.path.exists(pdf_path):
        os.remove(pdf_path)

if __name__ == "__main__":
    main()
