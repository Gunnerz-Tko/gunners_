#!/usr/bin/env python3
import os
import json
import requests
import pdfplumber
from datetime import datetime
import re
import logging

import requests
from bs4 import BeautifulSoup
import re
import json
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fetch_hanmoto_data(isbn):
    """
    Fetch book details from Hanmoto using ISBN
    Returns: {title, author, publisher} or None if not found
    """
    try:
        isbn_clean = isbn.replace('-', '')
        url = f'https://www.hanmoto.com/bd/isbn/{isbn_clean}'
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=5)
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract title
        title = None
        title_elem = soup.find('h1', class_='bookTitle')
        if title_elem:
            title = title_elem.get_text(strip=True)
        
        # Extract author (Ëëó)
        author = "-"
        author_elem = soup.find('dt', string=re.compile(r'Ëëó'))
        if author_elem:
            author_dd = author_elem.find_next('dd')
            if author_dd:
                author = author_dd.get_text(strip=True)
        
        # Extract publisher (Áô∫Ë°å)
        publisher = "-"
        publisher_elem = soup.find('dt', string=re.compile(r'Áô∫Ë°å'))
        if publisher_elem:
            publisher_dd = publisher_elem.find_next('dd')
            if publisher_dd:
                publisher = publisher_dd.get_text(strip=True)
        
        if title:
            return {
                'title': title,
                'author': author,
                'publisher': publisher
            }
        
        return None
        
    except Exception as e:
        logger.warning(f"Error fetching Hanmoto data for ISBN {isbn}: {e}")
        return None
        
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

TOHAN_PDF_URL = "https://www.tohan.jp/wp/wp-content/uploads/2026/02/202601.pdf"

GENRES = [
    "Á∑èÂêà",
    "ÊñáËä∏Êõ∏",
    "„Éé„É≥„Éï„Ç£„ÇØ„Ç∑„Éß„É≥„Éª„É©„Ç§„Éà„Ç®„ÉÉ„Çª„Ç§",
    "„Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É°„É≥„Éà",
    "„Éì„Ç∏„Éç„ÇπÊõ∏",
    "Ë∂£Âë≥ÂÆüÁî®Êõ∏",
    "ÁîüÊ¥ªÂÆüÁî®Êõ∏",
    "ÂÖêÁ´•Êõ∏",
    "„Éé„Éô„É´„Çπ",
    "Êñ∞Êõ∏",
    "ÊñáÂ∫´",
    "„Ç≥„Éü„ÉÉ„ÇØ„Çπ"
]

def download_tohan_pdf(url):
    """Download Tohan PDF"""
    print("üì• Downloading Tohan PDF...")
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        pdf_path = '/tmp/tohan.pdf'
        with open(pdf_path, 'wb') as f:
            f.write(response.content)
        
        print(f"‚úÖ PDF downloaded ({len(response.content)} bytes)\n")
        return pdf_path
    
    except Exception as e:
        logger.error(f"Error downloading PDF: {e}")
        return None

def parse_tohan_pdf(pdf_path):
    """Parse Tohan PDF and extract rankings from text"""
    print("üìñ Parsing Tohan PDF...\n")
    
    data = {
        "updated": datetime.now().isoformat() + "Z",
        "source": "tohan.jp",
        "genres": {}
    }
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"Total pages: {len(pdf.pages)}\n")
            
            # Extract all text
            full_text = ""
            for page in pdf.pages:
                full_text += page.extract_text() + "\n"
            
            # Parse each genre
            for genre in GENRES:
                print(f"üîç Extracting „Äê{genre}„Äë...")
                
                genre_pattern = f"„Äê{genre}„Äë"
                
                if genre_pattern not in full_text:
                    print(f"   ‚ö†Ô∏è  Not found\n")
                    continue
                
                # Find start and end of genre section
                start_idx = full_text.find(genre_pattern)
                
                end_idx = len(full_text)
                for next_genre in GENRES:
                    if next_genre != genre:
                        next_idx = full_text.find(f"„Äê{next_genre}„Äë", start_idx + 1)
                        if next_idx != -1 and next_idx < end_idx:
                            end_idx = next_idx
                
                genre_section = full_text[start_idx:end_idx]
                
                # Parse books
                books = parse_genre_section(genre_section)
                
                if books:
                    data["genres"][genre] = books
                    print(f"   ‚úÖ {len(books)} books extracted\n")
                else:
                    print(f"   ‚ö†Ô∏è  No books found\n")
        
        return data
    
    except Exception as e:
        logger.error(f"Error parsing PDF: {e}")
        import traceback
        traceback.print_exc()
        return None

def parse_genre_section(section_text):
    """Parse a genre section and extract books"""
    books = []
    lines = [line.rstrip() for line in section_text.split('\n')]
    
    # Find header line with column names (if it exists)
    header_idx = -1
    for i, line in enumerate(lines):
        if 'Êõ∏' in line and 'Âêç' in line:
            header_idx = i
            break
    
    # If no header found, start from first line after genre marker
    if header_idx == -1:
        # Find the genre marker line
        for i, line in enumerate(lines):
            if '„Äê' in line and '„Äë' in line:
                header_idx = i
                break
    
    if header_idx == -1:
        return []
    
    # Parse data lines starting after header/genre marker
    i = header_idx + 1
    found_books = 0
    
    while i < len(lines) and found_books < 10:
        line = lines[i].strip()
        
        # Skip empty lines at the beginning
        if not line:
            i += 1
            continue
        
        # Look for rank (1-10) at start
        match = re.match(r'^(\d+)\s+', line)
        
        if not match:
            i += 1
            continue
        
        rank = int(match.group(1))
        # Only process ranks 1-10, in order
        if rank != found_books + 1:
            i += 1
            continue
        
        # Collect all lines for this book entry
        book_data = [line]
        i += 1
        
        # Continue until next rank or end
        while i < len(lines):
            next_line = lines[i]
            
            # Stop at next rank
            if re.match(r'^(\d+)\s+', next_line):
                break
            
            # Stop at genre marker
            if '„Äê' in next_line and '„Äë' in next_line:
                break
            
            # Stop at page marker
            if '„Éà„Éº„Éè„É≥' in next_line or 'ÊúàÈñì„Éô„Çπ„Éà„Çª„É©„Éº' in next_line:
                break
            
            # Add non-empty lines
            if next_line.strip():
                book_data.append(next_line)
            
            i += 1
        
        # Parse the book data
        book = parse_book_entry(book_data, rank)
        if book:
            books.append(book)
            found_books += 1
            print(f"      ‚úì Rank {rank}: {book['title'][:50]}")
    
    return books

# List of known publishers
# List of known publishers
PUBLISHERS = [
    "SB„ÇØ„É™„Ç®„Ç§„ÉÜ„Ç£„Éñ", "KADOKAWA", "Âπ∏Á¶è„ÅÆÁßëÂ≠¶Âá∫Áâà", "„Ç™„É¨„É≥„Ç∏„Éö„Éº„Ç∏", "Â∞èÂ≠¶È§®",
    "Á•ûÂÆÆÈ§®", "Gakken", "ÊúùÊó•Êñ∞ËÅûÂá∫Áâà", "„ÉØ„É≥ÔΩ•„Éë„Éñ„É™„ÉÉ„Ç∑„É≥„Ç∞", "ÂπªÂÜ¨Ëàé",
    "Êó•Êú¨ÁµåÊ∏àÊñ∞ËÅûÂá∫Áâà", "È´òÊ©ãÊõ∏Â∫ó", "„Å®„Åç„ÇèÁ∑èÂêà„Çµ„Éº„Éì„Çπ", "„Çµ„É≥„ÇØ„ÉÅ„É•„Ç¢„É™Âá∫Áâà",
    "1‰∏áÂπ¥Â†ÇÂá∫Áâà", "PHPÁ†îÁ©∂ÊâÄ", "ÊØéÊó•Êñ∞ËÅûÂá∫Áâà", "Êó•ÁµåBP", "„Éñ„É©„Ç¶„É≥„Ç∫„Éñ„ÉÉ„ÇØ„Çπ",
    "„Çπ„Ç§„ÉÉ„ÉÅÔΩ•„Éë„Éñ„É™„ÉÉ„Ç∑„É≥„Ç∞", "„Åô„Å∞„ÇãËàé", "„Çµ„É≥„Éû„Éº„ÇØÂá∫Áâà", "„ÉØ„Éã„Éñ„ÉÉ„ÇØ„Çπ",
    "„Éû„Ç¨„Ç∏„É≥„Éè„Ç¶„Çπ", "Á¶èÈü≥È§®Êõ∏Â∫ó", "Â≤©Â¥éÊõ∏Â∫ó", "„Éè„Éº„Éë„Éº„Ç≥„É™„É≥„Ç∫ÔΩ•„Ç∏„É£„Éë„É≥",
    "ÊñáËóùÊò•Áßã", "Êñ∞ÊΩÆÁ§æ", "ÂèåËëâÁ§æ", "È£õÈ≥•Êñ∞Á§æ", "Ë¨õË´áÁ§æ", "Êù±‰∫¨ÂâµÂÖÉÁ§æ",
    "ÂÆùÂ≥∂Á§æ", "„ÉÄ„Ç§„É§„É¢„É≥„ÉâÁ§æ", "Êù±Ê¥ãÁµåÊ∏àÊñ∞Â†±Á§æ", "ÊúùÊó•Êñ∞ËÅûÂá∫Áâà", "Êñ∞ÊòüÂá∫ÁâàÁ§æ",
    "‰∏≠Â§ÆÂÖ¨Ë´ñÊñ∞Á§æ", "ÈõÜËã±Á§æ", "ÂÖâÊñáÁ§æ", "„ÇØ„É©„Éº„Ç±„É≥„Ç≥„Éü„ÉÉ„ÇØ„Çπ", "NHKÂá∫Áâà", "„Çπ„Ç§„ÉÉ„ÉÅÔΩ•„Éë„Éñ"
]

def parse_book_entry(lines, rank):
    """Parse a single book entry from PDF"""
    full_text = ' '.join(line.strip() for line in lines if line.strip())
    full_text = re.sub(r'^(\d+)\s+', '', full_text).strip()
    
    if not full_text:
        return None
    
    # Extract ISBN
    isbn = ""
    isbn_match = re.search(r'(978[\d\-]{10,})', full_text)
    if isbn_match:
        isbn = isbn_match.group(1)
        full_text = full_text[:isbn_match.start()].strip() + ' ' + full_text[isbn_match.end():].strip()
        full_text = full_text.strip()
    
    # Extract price
    price = extract_price(full_text)
    if price != "-":
        full_text = re.sub(r'\b' + re.escape(price) + r'\b', '', full_text).strip()
    
    # What remains is title, author, publisher
    # For now, use full_text as title
    title = full_text.strip()
    
    if not title:
        return None
    
    return {
        "rank": rank,
        "title": title,
        "author": "-",
        "publisher": "-",
        "price": price,
        "isbn": isbn
    }

def extract_price(text):
    """Extract price from text"""
    price_match = re.search(r'\b([\d]{1,3}(?:,\d{3})*|\d{3})\b(?![\d\-])', text)
    if price_match:
        price_candidate = price_match.group(1)
        if ',' in price_candidate or (len(price_candidate.replace(',', '')) <= 3):
            return price_candidate
    return "-"

def correct_overall_from_other_genres(data):
    """Not needed - OVERALL tab removed"""
    return data
    
    overall_books = data["genres"]["Á∑èÂêà"]
    
    # Build a reference map from all other genres
    reference_map = {}  # title -> book data
    
    for genre, books in data["genres"].items():
        if genre == "Á∑èÂêà":
            continue
        
        for book in books:
            title = book["title"].lower().strip()
            # Use this genre's data as reference if not already set
            if title not in reference_map:
                reference_map[title] = book.copy()
    
    # Correct overall books using reference map
    corrected_books = []
    for book in overall_books:
        title = book["title"].lower().strip()
        
        # Check if this title exists in other genres
        if title in reference_map:
            ref_book = reference_map[title]
            # Use reference data to fill in missing fields
            corrected = {
                "rank": book["rank"],
                "title": ref_book["title"],  # Use correct title from other genre
                "author": ref_book["author"] if ref_book["author"] != "-" else book["author"],
                "publisher": ref_book["publisher"] if ref_book["publisher"] != "-" else book["publisher"],
                "price": ref_book["price"] if ref_book["price"] != "-" else book["price"],
                "isbn": ref_book["isbn"] if ref_book["isbn"] != "-" else book["isbn"]
            }
            corrected_books.append(corrected)
        else:
            # Keep original if no reference found
            corrected_books.append(book)
    
    data["genres"]["Á∑èÂêà"] = corrected_books
    return data

def main():
    print("üìö Starting Tohan PDF Scraper...\n")
    
    pdf_path = download_tohan_pdf(TOHAN_PDF_URL)
    if not pdf_path:
        return
    
    data = parse_tohan_pdf(pdf_path)
    if not data:
        return
    
    # Correct OVERALL genre using other genres
    data = correct_overall_from_other_genres(data)
    
    # Save data.js
    try:
        js_content = f"const oricon_data = {json.dumps(data, ensure_ascii=False, indent=2)};\n"
        
        with open('data.js', 'w', encoding='utf-8') as f:
            f.write(js_content)
        
        print(f"‚úÖ Successfully saved data.js")
        print(f"üìä Total genres: {len(data['genres'])}")
        
        total_books = 0
        for genre, books in data['genres'].items():
            print(f"   - {genre}: {len(books)} books")
            total_books += len(books)
        
        print(f"\nüìà Total books scraped: {total_books}")
    
    except Exception as e:
        logger.error(f"Error saving data.js: {e}")
    
    # Cleanup
    if os.path.exists(pdf_path):
        os.remove(pdf_path)

if __name__ == "__main__":
    main()
